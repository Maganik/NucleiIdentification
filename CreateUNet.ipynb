{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des librairies necessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import time\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D,Cropping2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import imageio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition des fonctions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    folders = []\n",
    "    training_set = dict()\n",
    "    idx = 0\n",
    "\n",
    "    p = pathlib.Path(\"DATA/train/\")\n",
    "\n",
    "    #Looking for all the folders existing in train folder\n",
    "    for i in p.glob('*'):\n",
    "\n",
    "        path = \"DATA/train/\"\n",
    "        path += i.name;\n",
    "\n",
    "        f_path = pathlib.Path(path)\n",
    "        folders.append(f_path)\n",
    "\n",
    "\n",
    "    for p in folders:\n",
    "\n",
    "        im_p = str(p)\n",
    "        im_p += \"/images/\"\n",
    "        ma_p = str(p)\n",
    "        ma_p += \"/masks/\"\n",
    "\n",
    "        #First, we set the paths for the image and the masks\n",
    "\n",
    "        im_path = pathlib.Path(im_p)\n",
    "        ma_path = pathlib.Path(ma_p)\n",
    "\n",
    "\n",
    "        #Then, we read the image\n",
    "        name =\"\"\n",
    "        for i in im_path.glob('*.png'):\n",
    "            name = i.name\n",
    "        im_p += name\n",
    "        im = rgb2gray(imageio.imread(im_p))\n",
    "\n",
    "\n",
    "        masks = []\n",
    "        #Then, we read every sub mask\n",
    "        maskp = ma_p\n",
    "        for i in ma_path.glob('*.png'):\n",
    "            name =\"\"\n",
    "            name = i.name\n",
    "            ma_p = maskp\n",
    "            ma_p += name\n",
    "            sub_mask = imageio.imread(ma_p)\n",
    "            masks.append(sub_mask)\n",
    "\n",
    "        mask = masks[0]\n",
    "        for m in masks: \n",
    "            mask = np.where((mask + m) > 200, 255, 0)\n",
    "            #Finaly, we regroup every sub mask in a single one.\n",
    "\n",
    "\n",
    "        #We add the image and its mask to the dictionary    \n",
    "        training_set[idx] = (im, mask)\n",
    "        idx += 1\n",
    "    return idx, training_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perlin(x,y,seed=0):\n",
    "    # permutation table\n",
    "    np.random.seed(seed)\n",
    "    p = np.arange(256,dtype=int)\n",
    "    np.random.shuffle(p)\n",
    "    p = np.stack([p,p]).flatten()\n",
    "    # coordinates of the top-left\n",
    "    xi = x.astype(int)\n",
    "    yi = y.astype(int)\n",
    "    # internal coordinates\n",
    "    xf = x - xi\n",
    "    yf = y - yi\n",
    "    # fade factors\n",
    "    u = fade(xf)\n",
    "    v = fade(yf)\n",
    "    # noise components\n",
    "    n00 = gradient(p[p[xi]+yi],xf,yf)\n",
    "    n01 = gradient(p[p[xi]+yi+1],xf,yf-1)\n",
    "    n11 = gradient(p[p[xi+1]+yi+1],xf-1,yf-1)\n",
    "    n10 = gradient(p[p[xi+1]+yi],xf-1,yf)\n",
    "    # combine noises\n",
    "    x1 = lerp(n00,n10,u)\n",
    "    x2 = lerp(n01,n11,u) \n",
    "    return lerp(x1,x2,v) \n",
    "\n",
    "def lerp(a,b,x):\n",
    "    \"linear interpolation\"\n",
    "    return a + x * (b-a)\n",
    "\n",
    "def fade(t):\n",
    "    \"6t^5 - 15t^4 + 10t^3\"\n",
    "    return 6 * t**5 - 15 * t**4 + 10 * t**3\n",
    "\n",
    "def gradient(h,x,y):\n",
    "    \"grad converts h to the right gradient vector and return the dot product with (x,y)\"\n",
    "    vectors = np.array([[0,1],[0,-1],[1,0],[-1,0]])\n",
    "    g = vectors[h%4]\n",
    "    return g[:,:,0] * x + g[:,:,1] * y\n",
    "\n",
    "lin = np.linspace(0,2,256,endpoint=False)\n",
    "x,y = np.meshgrid(lin,lin)\n",
    "\n",
    "im_p = perlin(x,y,seed=random.randint(5,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_transform(image, alpha, sigma, alpha_affine, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003]_ (with modifications).\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "         Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "         Proc. of the International Conference on Document Analysis and\n",
    "         Recognition, 2003.\n",
    "\n",
    "     Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "    \n",
    "    # Random affine\n",
    "    center_square = np.float32(shape_size) // 2\n",
    "    square_size = min(shape_size) // 3\n",
    "    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
    "    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    image = cv2.warpAffine(image, M, shape_size, image,borderMode=cv2.BORDER_REFLECT_101)\n",
    "    \n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dz = np.zeros_like(dx)\n",
    "\n",
    "    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    "\n",
    "    return map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)\n",
    "\n",
    "\n",
    "\n",
    "def draw_grid(im, grid_size):\n",
    "    # Draw grid lines\n",
    "    for i in range(0, im.shape[1], grid_size):\n",
    "        cv2.line(im, (i, 0), (i, im.shape[0]), color=(255,))\n",
    "    for j in range(0, im.shape[0], grid_size):\n",
    "        cv2.line(im, (0, j), (im.shape[1], j), color=(255,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createUnet(height,width,depth,kernel):\n",
    "    inputLayer = Input(( height, width, depth))\n",
    "\n",
    "    normalise = Lambda(lambda x: x / 255) (inputLayer)\n",
    "\n",
    "    conv11 = Conv2D(16, kernel, padding='valid', activation='relu', kernel_initializer='random_uniform')(normalise)\n",
    "    conv12 = Conv2D(16, kernel, padding='valid', activation='relu', kernel_initializer='random_uniform')(conv11)\n",
    "    pool1 = MaxPooling2D((2,2))(conv12)\n",
    "    conv21 = Conv2D(32, kernel, padding='valid', activation='relu', kernel_initializer='random_uniform')(pool1)\n",
    "    conv22 = Conv2D(32, kernel, padding='valid', activation='relu', kernel_initializer='random_uniform')(conv21)\n",
    "    pool2 = MaxPooling2D((2,2))(conv22)\n",
    "    conv31 = Conv2D(64, kernel, padding='valid', activation='relu', kernel_initializer='random_uniform')(pool2)\n",
    "    conv32 = Conv2D(64, kernel, padding='valid', activation='relu', kernel_initializer='random_uniform')(conv31)\n",
    "    pool3 = MaxPooling2D((2,2))(conv32)\n",
    "    conv41 = Conv2D(128, kernel, padding='valid', activation='relu', kernel_initializer='random_uniform')(pool3)\n",
    "    conv42 = Conv2D(128, kernel, padding='valid', activation='relu', kernel_initializer='random_uniform')(conv41)\n",
    "    pool4 = MaxPooling2D((2,2))(conv42)\n",
    "    conv51 = Conv2D(256, kernel, padding='valid', activation='relu', kernel_initializer='random_uniform')(pool4)\n",
    "    conv52 = Conv2D(256, kernel, padding='valid', activation='relu', kernel_initializer='random_uniform')(conv51)\n",
    "\n",
    "\n",
    "\n",
    "    upconv1 = UpSampling2D(size=(2, 2))(conv52)\n",
    "    ch, cw = get_crop_shape(conv42, upconv1)\n",
    "    cropconv4 = Cropping2D(cropping=(ch,cw))(conv42)\n",
    "    up1   = concatenate([upconv1, cropconv4], axis=3)\n",
    "    conv61 = Conv2D(128, kernel, padding=\"valid\", activation=\"relu\", kernel_initializer='random_uniform')(up1)\n",
    "    conv62 = Conv2D(128, kernel, padding=\"valid\", activation=\"relu\", kernel_initializer='random_uniform')(conv61)\n",
    "\n",
    "\n",
    "    upconv2 = UpSampling2D(size=(2, 2))(conv62)\n",
    "    ch, cw = get_crop_shape(conv32, upconv2)\n",
    "    cropconv3 = Cropping2D(cropping=(ch,cw))(conv32)\n",
    "    up2  = concatenate([upconv2, cropconv3], axis=3)\n",
    "    conv71 = Conv2D(64, kernel, padding=\"valid\", activation=\"relu\", kernel_initializer='random_uniform')(up2)\n",
    "    conv72 = Conv2D(64, kernel, padding=\"valid\", activation=\"relu\", kernel_initializer='random_uniform')(conv71)\n",
    "\n",
    "\n",
    "    upconv3 = UpSampling2D(size=(2, 2))(conv72)\n",
    "    ch, cw = get_crop_shape(conv22, upconv3)\n",
    "    cropconv2 = Cropping2D(cropping=(ch,cw))(conv22)\n",
    "    up3  = concatenate([upconv3, cropconv2], axis=3)\n",
    "    conv81 = Conv2D(32, kernel, padding=\"valid\", activation=\"relu\", kernel_initializer='random_uniform')(up3)\n",
    "    conv82 = Conv2D(32, kernel, padding=\"valid\", activation=\"relu\", kernel_initializer='random_uniform')(conv81)\n",
    "\n",
    "    upconv4 = UpSampling2D(size=(2, 2))(conv82)\n",
    "    ch, cw = get_crop_shape(conv12, upconv4)\n",
    "    cropconv1 = Cropping2D(cropping=(ch,cw))(conv12)\n",
    "    up4  = concatenate([upconv4, cropconv1], axis=3)\n",
    "    conv91 = Conv2D(16, kernel, padding=\"valid\", activation=\"relu\", kernel_initializer='random_uniform')(up4)\n",
    "    conv92 = Conv2D(16, kernel, padding=\"valid\", activation=\"relu\", kernel_initializer='random_uniform')(conv91)\n",
    "\n",
    "    outputconv = Conv2D(1, (1, 1), activation=\"sigmoid\")(conv92)\n",
    "\n",
    "\n",
    "    u_net = Model(inputs= inputLayer, outputs = outputconv)\n",
    "    return u_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "        return v\n",
    "    return v / norm\n",
    "\n",
    "def elastoperlin_transformation(im ,mask,elast, perl, shape):\n",
    "    lina = np.linspace(0,1,shape[0],endpoint=False)\n",
    "    linb = np.linspace(0,1,shape[1],endpoint=False)\n",
    "    a,b = np.meshgrid(linb,lina)\n",
    "    if perl:\n",
    "        im_p = perlin(a,b,seed=random.randint(1,20000))\n",
    "        im_p = np.abs(im_p)\n",
    "        im_p = np.array((normalize(im_p)*255), dtype = np.uint8)\n",
    "        #im_p = np.concatenate((im_p[...,None], im_p[...,None],im_p[...,None]), axis=2)\n",
    "        im = im + im_p\n",
    "    if elast:\n",
    "        im = np.concatenate((im[...,None], mask[...,None]), axis=2)\n",
    "        im = elastic_transform(im,im.shape[1]*2,im.shape[1]*0.1,im.shape[1]*0.08) \n",
    "        mask = im[:,:,1:]\n",
    "        im = im[:,:,0]\n",
    "    im = cv2.copyMakeBorder(im,92,92,92,92,cv2.BORDER_REFLECT) \n",
    "    # 92 représente la marge rognée par les convolutions successives\n",
    "    return im, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imaskdivision(im,mask,shape, padding):\n",
    "    # padding est la taille de la marge non masquée (92)\n",
    "    # shape est la shape maximale d'un echantillon (132,132) dans notre cas\n",
    "    ims = []\n",
    "    masks = []\n",
    "    widthDiv = int(np.ceil(float(im.shape[0]-(2*padding))/float(shape[0])))\n",
    "    heightDiv = int(np.ceil(float(im.shape[1]-(2*padding))/float(shape[1])))\n",
    "    for i in range(widthDiv):\n",
    "        for j in range(heightDiv):\n",
    "            if i == widthDiv-1:\n",
    "                sw = im.shape[0]-(shape[0]+padding)\n",
    "            else:\n",
    "                sw = padding + shape[0]*i\n",
    "                \n",
    "            if j == heightDiv-1:\n",
    "                sh = im.shape[1]-(shape[1]+padding)\n",
    "            else:\n",
    "                sh = padding + shape[1]*j\n",
    "            \n",
    "            imseg = im[sw-padding:sw+shape[0]+padding,sh-padding:sh+shape[1]+padding]\n",
    "            ims.append(imseg)\n",
    "            maskseg = mask[sw-padding:sw+shape[0]-padding,sh-padding:sh+shape[1]-padding]\n",
    "            maskseg = (maskseg>0)\n",
    "            masks.append(maskseg)\n",
    "    return ims,masks\n",
    "\n",
    "def dataAugmentation(im, mask, rate):\n",
    "    train = []\n",
    "    labels = []\n",
    "    for i in range(rate):\n",
    "        imtemp, masktemp = elastoperlin_transformation(im, mask, True, False,np.shape(im))\n",
    "        ims, masks = imaskdivision(imtemp, masktemp,(132,132),92)\n",
    "        for image in ims:\n",
    "            train.append(image)\n",
    "        for m in masks:\n",
    "            labels.append(m)\n",
    "    return train , labels        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def trainDatas(training_set, nbIms,rate):\n",
    "    train = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(nbIms):\n",
    "        im, mask = training_set[i]\n",
    "        ims , masks = dataAugmentation(im,mask,rate)\n",
    "        print i, \" images traitées\"\n",
    "        for image in ims:\n",
    "            train.append(np.reshape(image,(316,316,1)))\n",
    "        for m in masks:\n",
    "            labels.append(np.reshape(m, (132,132,1)))\n",
    "    train = np.array(train)\n",
    "    labels = np.array(labels)\n",
    "    return train ,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_shape(target, refer):\n",
    "        # width, the 3rd dimension\n",
    "        cw = (target.get_shape()[2] - refer.get_shape()[2]).value\n",
    "        assert (cw >= 0)\n",
    "        if cw % 2 != 0:\n",
    "            cw1, cw2 = int(cw/2), int(cw/2) + 1\n",
    "        else:\n",
    "            cw1, cw2 = int(cw/2), int(cw/2)\n",
    "        # height, the 2nd dimension\n",
    "        ch = (target.get_shape()[1] - refer.get_shape()[1]).value\n",
    "        assert (ch >= 0)\n",
    "        if ch % 2 != 0:\n",
    "            ch1, ch2 = int(ch/2), int(ch/2) + 1\n",
    "        else:\n",
    "            ch1, ch2 = int(ch/2), int(ch/2)\n",
    "\n",
    "        return (ch1, ch2), (cw1, cw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightLoss(y_true, y_pred):\n",
    "    [mask, weight] = tf.unstack(y_true, 2, axis=3)\n",
    "    weighted_pred = y_pred*weight\n",
    "    weighted_mask = mask*weight\n",
    "    return binary_crossentropy(weighted_mask, weighted_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des images et creation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbImagesUsables,trainingData = getData()\n",
    "imagesUsed = 15\n",
    "augDim = 2\n",
    "train, labels = trainDatas(trainingData,imagesUsed,augDim)\n",
    "del trainingData\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creation  du réseau et définition de la perte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des variables\n",
    "height = 316\n",
    "width = 316\n",
    "depth = 1\n",
    "kernel= (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_net = createUnet(height,width,depth,kernel)\n",
    "u_net.compile(optimizer='adam', loss='binary_crossentropy', metrics =[metrics.binary_accuracy])\n",
    "u_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(patience=4, verbose=1)\n",
    "checkpointer = ModelCheckpoint('model-dsbowl2018.h5', verbose=1, save_best_only=True)\n",
    "results = u_net.fit(train, labels, validation_split=0.2, batch_size=4, epochs=15, \n",
    "                    callbacks=[earlystopper, checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction et visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = u_net.predict(train, batch_size = 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in range(20):\n",
    "    \n",
    "    plt.figure(1)\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(train[k,92:224,92:224,0], interpolation=\"nearest\",cmap =\"Greys\")\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(np.reshape((p_test[k]),(132,132)), interpolation=\"nearest\",cmap =\"Greys\")\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(np.reshape(labels[k],(132,132)),interpolation=\"nearest\",cmap =\"Greys\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
